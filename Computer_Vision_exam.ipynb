{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing important libraries"
      ],
      "metadata": {
        "id": "lGyyjaj3D4cM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score, classification_report\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy, AUC\n",
        "from tensorflow.keras.applications import InceptionV3  # Import InceptionV3 as GoogleNet is not directly available\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n"
      ],
      "metadata": {
        "id": "OB7Jw3GlEFA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TZN0MlqpEFYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1pefcyKB_LA",
        "outputId": "248da081-61b0-446d-d7d3-49ce2699ecbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN Implementation**\n",
        "\n",
        "1. **GoogleNet Model**"
      ],
      "metadata": {
        "id": "IpZnIui6EgHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "input_folder = '/content/drive/MyDrive/yolo_dataset'  # Change this to your input folder path\n",
        "output_folder = os.path.join(input_folder, 'new_output')\n",
        "obj_names_path = '/content/drive/MyDrive/yolo_dataset/obj.names'  # Change this to your obj_names file path\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Function to read class labels from obj_names file\n",
        "def read_class_labels(obj_names_path):\n",
        "    with open(obj_names_path, 'r') as file:\n",
        "        class_labels = file.read().splitlines()\n",
        "    # Ensure \"saloon cars\" is first and \"motorcycles\" is second\n",
        "    if class_labels[0] != \"saloon cars\":\n",
        "        class_labels[0], class_labels[1] = class_labels[1], class_labels[0]\n",
        "    return class_labels\n",
        "\n",
        "# Function to draw bounding boxes and labels on the image\n",
        "def draw_bounding_boxes(image_path, annotation_path, class_labels):\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    with open(annotation_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            class_id, x_center, y_center, box_width, box_height = map(float, line.strip().split())\n",
        "            class_id = int(class_id)\n",
        "            x_center *= width\n",
        "            y_center *= height\n",
        "            box_width *= width\n",
        "            box_height *= height\n",
        "\n",
        "            x_min = int(x_center - (box_width / 2))\n",
        "            y_min = int(y_center - (box_height / 2))\n",
        "            x_max = int(x_center + (box_width / 2))\n",
        "            y_max = int(x_center + (box_width / 2))\n",
        "\n",
        "            # Draw the rectangle on the image\n",
        "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
        "\n",
        "            # Put class label text on the image\n",
        "            label = class_labels[class_id]\n",
        "            cv2.putText(image, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Read class labels from obj_names file\n",
        "class_labels = read_class_labels(obj_names_path)\n",
        "\n",
        "# Process each image and its corresponding annotation\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.PNG'):\n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "        annotation_path = os.path.join(input_folder, filename.replace('.PNG', '.txt'))\n",
        "\n",
        "        if os.path.exists(annotation_path):\n",
        "            annotated_image = draw_bounding_boxes(image_path, annotation_path, class_labels)\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "            cv2.imwrite(output_path, annotated_image)\n",
        "            print(f'Saved annotated image: {output_path}')\n",
        "\n",
        "print('Processing complete.')\n",
        "\n",
        "# Create separate folders for each class if they don't exist\n",
        "saloon_cars_folder = os.path.join(output_folder, 'saloon_cars')\n",
        "motorcycles_folder = os.path.join(output_folder, 'motorcycles')\n",
        "\n",
        "if not os.path.exists(saloon_cars_folder):\n",
        "    os.makedirs(saloon_cars_folder)\n",
        "\n",
        "if not os.path.exists(motorcycles_folder):\n",
        "    os.makedirs(motorcycles_folder)\n",
        "\n",
        "# Function to crop and save image patches\n",
        "def crop_and_save(image, class_id, x_min, y_min, x_max, y_max, filename, count):\n",
        "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "    if class_id == 0:\n",
        "        output_path = os.path.join(saloon_cars_folder, f'saloon_cars_{count}_{filename}')\n",
        "    elif class_id == 1:\n",
        "        output_path = os.path.join(motorcycles_folder, f'motorcycles_{count}_{filename}')\n",
        "    cv2.imwrite(output_path, cropped_image)\n",
        "    print(f'Saved cropped image: {output_path}')\n",
        "\n",
        "# Initialize a counter for each class to ensure unique filenames\n",
        "saloon_cars_count = 0\n",
        "motorcycles_count = 0\n",
        "\n",
        "# Process each image and its corresponding annotation to crop and save patches\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.PNG'):\n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "        annotation_path = os.path.join(input_folder, filename.replace('.PNG', '.txt'))\n",
        "\n",
        "        if os.path.exists(annotation_path):\n",
        "            image = cv2.imread(image_path)\n",
        "            height, width, _ = image.shape\n",
        "\n",
        "            with open(annotation_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    class_id, x_center, y_center, box_width, box_height = map(float, line.strip().split())\n",
        "                    class_id = int(class_id)\n",
        "                    x_center *= width\n",
        "                    y_center *= height\n",
        "                    box_width *= width\n",
        "                    box_height *= height\n",
        "\n",
        "                    x_min = int(x_center - (box_width / 2))\n",
        "                    y_min = int(y_center - (box_height / 2))\n",
        "                    x_max = int(x_center + (box_width / 2))\n",
        "                    y_max = int(x_center + (box_width / 2))\n",
        "\n",
        "                    # Crop and save the image patch\n",
        "                    if class_id == 0:\n",
        "                        saloon_cars_count += 1\n",
        "                        crop_and_save(image, class_id, x_min, y_min, x_max, y_max, filename, saloon_cars_count)\n",
        "                    elif class_id == 1:\n",
        "                        motorcycles_count += 1\n",
        "                        crop_and_save(image, class_id, x_min, y_min, x_max, y_max, filename, motorcycles_count)\n",
        "\n",
        "print('Cropping and saving complete.')\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "dataset_new = image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/yolo_dataset/yolo_dataset/images',\n",
        "    batch_size=32,\n",
        "    image_size=(224, 224),\n",
        "    label_mode='int'\n",
        ")\n",
        "\n",
        "scaled_data = dataset_new.map(lambda x, y: (preprocess_input(x), y))\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "dataset_size = tf.data.experimental.cardinality(scaled_data).numpy()\n",
        "train_size = int(0.8 * dataset_size)\n",
        "test_size = dataset_size - train_size\n",
        "\n",
        "train = scaled_data.take(train_size)\n",
        "test = scaled_data.skip(train_size).take(test_size)\n",
        "\n",
        "# Define the GoogleNet (Inception V1) model architecture using InceptionV3 as base\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Add custom layers on top of the base model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model with limited batches\n",
        "hist = model.fit(train, validation_data=test, epochs=5, steps_per_epoch=15, validation_steps=15)\n",
        "\n",
        "# Model evaluation metrics\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Evaluate the model\n",
        "for batch in test.as_numpy_iterator():\n",
        "    X, y = batch\n",
        "    y_pred_batch = model.predict(X)\n",
        "    y_true.extend(y)\n",
        "    y_pred.extend(np.argmax(y_pred_batch, axis=1))\n",
        "\n",
        "# Convert y_true and y_pred to numpy arrays\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion Matrix:\\n{cm}')\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print AUC, precision, recall, and F1 score\n",
        "auc = roc_auc_score(y_true, y_pred, multi_class='ovo')\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "classification_rep = classification_report(y_true, y_pred, target_names=class_labels)\n",
        "\n",
        "print(f'AUC: {auc}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Classification Report:\\n{classification_rep}')\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "fig = plt.figure()\n",
        "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
        "plt.plot(hist.history['val_accuracy'], color='blue', label='val_accuracy')\n",
        "fig.suptitle('ACCURACY', fontsize)\n",
        "fig.suptitle('ACCURACY', fontsize=20)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss\n",
        "fig = plt.figure()\n",
        "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
        "plt.plot(hist.history['val_loss'], color='blue', label='val_loss')\n",
        "fig.suptitle('LOSS', fontsize=20)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ySRTnLtyDrUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **LeNet-5**"
      ],
      "metadata": {
        "id": "0bvoBTNpFV8S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "input_folder = '/content/drive/MyDrive/yolo_dataset'  # Change this to your input folder path\n",
        "output_folder = os.path.join(input_folder, 'new_output')\n",
        "obj_names_path = '/content/drive/MyDrive/yolo_dataset/obj.names'  # Change this to your obj_names file path\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Function to read class labels from obj_names file\n",
        "def read_class_labels(obj_names_path):\n",
        "    with open(obj_names_path, 'r') as file:\n",
        "        class_labels = file.read().splitlines()\n",
        "    # Ensure \"saloon cars\" is first and \"motorcycles\" is second\n",
        "    if class_labels[0] != \"saloon cars\":\n",
        "        class_labels[0], class_labels[1] = class_labels[1], class_labels[0]\n",
        "    return class_labels\n",
        "\n",
        "# Function to draw bounding boxes and labels on the image\n",
        "def draw_bounding_boxes(image_path, annotation_path, class_labels):\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    with open(annotation_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            class_id, x_center, y_center, box_width, box_height = map(float, line.strip().split())\n",
        "            class_id = int(class_id)\n",
        "            x_center *= width\n",
        "            y_center *= height\n",
        "            box_width *= width\n",
        "            box_height *= height\n",
        "\n",
        "            x_min = int(x_center - (box_width / 2))\n",
        "            y_min = int(y_center - (box_height / 2))\n",
        "            x_max = int(x_center + (box_width / 2))\n",
        "            y_max = int(y_center + (box_width / 2))\n",
        "\n",
        "            # Draw the rectangle on the image\n",
        "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
        "\n",
        "            # Put class label text on the image\n",
        "            label = class_labels[class_id]\n",
        "            cv2.putText(image, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Read class labels from obj_names file\n",
        "class_labels = read_class_labels(obj_names_path)\n",
        "\n",
        "# Process each image and its corresponding annotation\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.PNG'):\n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "        annotation_path = os.path.join(input_folder, filename.replace('.PNG', '.txt'))\n",
        "\n",
        "        if os.path.exists(annotation_path):\n",
        "            annotated_image = draw_bounding_boxes(image_path, annotation_path, class_labels)\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "            cv2.imwrite(output_path, annotated_image)\n",
        "            print(f'Saved annotated image: {output_path}')\n",
        "\n",
        "print('Processing complete.')\n",
        "\n",
        "# Create separate folders for each class if they don't exist\n",
        "saloon_cars_folder = os.path.join(output_folder, 'saloon_cars')\n",
        "motorcycles_folder = os.path.join(output_folder, 'motorcycles')\n",
        "\n",
        "if not os.path.exists(saloon_cars_folder):\n",
        "    os.makedirs(saloon_cars_folder)\n",
        "\n",
        "if not os.path.exists(motorcycles_folder):\n",
        "    os.makedirs(motorcycles_folder)\n",
        "\n",
        "# Function to crop and save image patches\n",
        "def crop_and_save(image, class_id, x_min, y_min, x_max, y_max, filename, count):\n",
        "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "    if class_id == 0:\n",
        "        output_path = os.path.join(saloon_cars_folder, f'saloon_cars_{count}_{filename}')\n",
        "    elif class_id == 1:\n",
        "        output_path = os.path.join(motorcycles_folder, f'motorcycles_{count}_{filename}')\n",
        "    cv2.imwrite(output_path, cropped_image)\n",
        "    print(f'Saved cropped image: {output_path}')\n",
        "\n",
        "# Initialize a counter for each class to ensure unique filenames\n",
        "saloon_cars_count = 0\n",
        "motorcycles_count = 0\n",
        "\n",
        "# Process each image and its corresponding annotation to crop and save patches\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.PNG'):\n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "        annotation_path = os.path.join(input_folder, filename.replace('.PNG', '.txt'))\n",
        "\n",
        "        if os.path.exists(annotation_path):\n",
        "            image = cv2.imread(image_path)\n",
        "            height, width, _ = image.shape\n",
        "\n",
        "            with open(annotation_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    class_id, x_center, y_center, box_width, box_height = map(float, line.strip().split())\n",
        "                    class_id = int(class_id)\n",
        "                    x_center *= width\n",
        "                    y_center *= height\n",
        "                    box_width *= width\n",
        "                    box_height *= height\n",
        "\n",
        "                    x_min = int(x_center - (box_width / 2))\n",
        "                    y_min = int(y_center - (box_height / 2))\n",
        "\n",
        "                    x_max = int(x_center + (box_width / 2))\n",
        "                    y_max = int(x_center + (box_width / 2))\n",
        "\n",
        "                    # Crop and save the image patch\n",
        "                    if class_id == 0:\n",
        "                        saloon_cars_count += 1\n",
        "                        crop_and_save(image, class_id, x_min, y_min, x_max, y_max, filename, saloon_cars_count)\n",
        "                    elif class_id == 1:\n",
        "                        motorcycles_count += 1\n",
        "                        crop_and_save(image, class_id, x_min, y_min, x_max, y_max, filename, motorcycles_count)\n",
        "\n",
        "print('Cropping and saving complete.')\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "dataset_new = image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/yolo_dataset/yolo_dataset/images',\n",
        "    batch_size=32,\n",
        "    image_size=(32, 32),  # LeNet-5 works with 32x32 images\n",
        "    label_mode='int'\n",
        ")\n",
        "\n",
        "scaled_data = dataset_new.map(lambda x, y: (x / 255.0, y))\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "dataset_size = tf.data.experimental.cardinality(scaled_data).numpy()\n",
        "train_size = int(0.8 * dataset_size)\n",
        "test_size = dataset_size - train_size\n",
        "\n",
        "train = scaled_data.take(train_size)\n",
        "test = scaled_data.skip(train_size).take(test_size)\n",
        "\n",
        "# Define the LeNet-5 model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(16, kernel_size=(5, 5), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(120, activation='relu'),\n",
        "    Dense(84, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "hist = model.fit(train, validation_data=test, epochs=5)\n",
        "\n",
        "# Model evaluation metrics\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Evaluate the model\n",
        "for batch in test.as_numpy_iterator():\n",
        "    X, y = batch\n",
        "    y_pred_batch = model.predict(X)\n",
        "    y_true.extend(y)\n",
        "    y_pred.extend(np.argmax(y_pred_batch, axis=1))\n",
        "\n",
        "# Convert y_true and y_pred to numpy arrays\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion Matrix:\\n{cm}')\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print AUC, precision, recall, and F1 score\n",
        "auc = roc_auc_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "classification_rep = classification_report(y_true, y_pred, target_names=class_labels)\n",
        "\n",
        "print(f'AUC: {auc}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Classification Report:\\n{classification_rep}')\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "fig = plt.figure()\n",
        "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
        "plt.plot(hist.history['val_accuracy'], color='blue', label='val_accuracy')\n",
        "fig.suptitle('ACCURACY', fontsize=20)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss\n",
        "fig = plt.figure()\n",
        "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
        "plt.plot(hist.history['val_loss'], color='blue', label='val_loss')\n",
        "fig.suptitle('LOSS', fontsize=20)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "9-jCbBOcDrX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **VGG-16**"
      ],
      "metadata": {
        "id": "BXTT9JkyGIiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "input_folder = '/content/drive/MyDrive/yolo_dataset'  # Change this to your input folder path\n",
        "output_folder = os.path.join(input_folder, 'new_output')\n",
        "obj_names_path = '/content/drive/MyDrive/yolo_dataset/obj.names'  # Change this to your obj_names file path\n",
        "\n",
        "# Create the output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Function to read class labels from obj_names file\n",
        "def read_class_labels(obj_names_path):\n",
        "    with open(obj_names_path, 'r') as file:\n",
        "        class_labels = file.read().splitlines()\n",
        "    # Ensure \"saloon cars\" is first and \"motorcycles\" is second\n",
        "    if class_labels[0] != \"saloon cars\":\n",
        "        class_labels[0], class_labels[1] = class_labels[1], class_labels[0]\n",
        "    return class_labels\n",
        "\n",
        "# Function to draw bounding boxes and labels on the image\n",
        "def draw_bounding_boxes(image_path, annotation_path, class_labels):\n",
        "    image = cv2.imread(image_path)\n",
        "    height, width, _ = image.shape\n",
        "\n",
        "    with open(annotation_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            class_id, x_center, y_center, box_width, box_height = map(float, line.strip().split())\n",
        "            class_id = int(class_id)\n",
        "            x_center *= width\n",
        "            y_center *= height\n",
        "            box_width *= width\n",
        "            box_height *= height\n",
        "\n",
        "            x_min = int(x_center - (box_width / 2))\n",
        "            y_min = int(x_center - (box_width / 2))\n",
        "            x_max = int(x_center + (box_width / 2))\n",
        "            y_max = int(y_center + (box_width / 2))\n",
        "\n",
        "            # Draw the rectangle on the image\n",
        "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (255, 0, 0), 2)\n",
        "\n",
        "            # Put class label text on the image\n",
        "            label = class_labels[class_id]\n",
        "            cv2.putText(image, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "    return image\n",
        "\n",
        "# Read class labels from obj_names file\n",
        "class_labels = read_class_labels(obj_names_path)\n",
        "\n",
        "# Process each image and its corresponding annotation\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.PNG'):\n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "        annotation_path = os.path.join(input_folder, filename.replace('.PNG', '.txt'))\n",
        "\n",
        "        if os.path.exists(annotation_path):\n",
        "            annotated_image = draw_bounding_boxes(image_path, annotation_path, class_labels)\n",
        "            output_path = os.path.join(output_folder, filename)\n",
        "            cv2.imwrite(output_path, annotated_image)\n",
        "            print(f'Saved annotated image: {output_path}')\n",
        "\n",
        "print('Processing complete.')\n",
        "\n",
        "# Create separate folders for each class if they don't exist\n",
        "saloon_cars_folder = os.path.join(output_folder, 'saloon_cars')\n",
        "motorcycles_folder = os.path.join(output_folder, 'motorcycles')\n",
        "\n",
        "if not os.path.exists(saloon_cars_folder):\n",
        "    os.makedirs(saloon_cars_folder)\n",
        "\n",
        "if not os.path.exists(motorcycles_folder):\n",
        "    os.makedirs(motorcycles_folder)\n",
        "\n",
        "# Function to crop and save image patches\n",
        "def crop_and_save(image, class_id, x_min, y_min, x_max, y_max, filename, count):\n",
        "    cropped_image = image[y_min:y_max, x_min:x_max]\n",
        "    if class_id == 0:\n",
        "        output_path = os.path.join(saloon_cars_folder, f'saloon_cars_{count}_{filename}')\n",
        "    elif class_id == 1:\n",
        "        output_path = os.path.join(motorcycles_folder, f'motorcycles_{count}_{filename}')\n",
        "    cv2.imwrite(output_path, cropped_image)\n",
        "    print(f'Saved cropped image: {output_path}')\n",
        "\n",
        "# Initialize a counter for each class to ensure unique filenames\n",
        "saloon_cars_count = 0\n",
        "motorcycles_count = 0\n",
        "\n",
        "# Process each image and its corresponding annotation to crop and save patches\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith('.PNG'):\n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "        annotation_path = os.path.join(input_folder, filename.replace('.PNG', '.txt'))\n",
        "\n",
        "        if os.path.exists(annotation_path):\n",
        "            image = cv2.imread(image_path)\n",
        "            height, width, _ = image.shape\n",
        "\n",
        "            with open(annotation_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    class_id, x_center, y_center, box_width, box_height = map(float, line.strip().split())\n",
        "                    class_id = int(class_id)\n",
        "                    x_center *= width\n",
        "                    y_center *= height\n",
        "                    box_width *= width\n",
        "                    box_height *= height\n",
        "\n",
        "                    x_min = int(x_center - (box_width / 2))\n",
        "                    y_min = int(x_center - (box_width / 2))\n",
        "                    x_max = int(x_center + (box_width / 2))\n",
        "                    y_max = int(y_center + (box_width / 2))\n",
        "\n",
        "                    # Crop and save the image patch\n",
        "                    if class_id == 0:\n",
        "                        saloon_cars_count += 1\n",
        "                        crop_and_save(image, class_id, x_min, y_min, x_max, y_max, filename, saloon_cars_count)\n",
        "                    elif class_id == 1:\n",
        "                        motorcycles_count += 1\n",
        "                        crop_and_save(image, class_id, x_min, y_min, x_max, y_max, filename, motorcycles_count)\n",
        "\n",
        "print('Cropping and saving complete.')\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "dataset_new = image_dataset_from_directory(\n",
        "    '/content/drive/MyDrive/yolo_dataset/yolo_dataset/images',\n",
        "    batch_size=32,\n",
        "    image_size=(224, 224),  # VGG16 works with 224x224 images\n",
        "    label_mode='int'\n",
        ")\n",
        "\n",
        "scaled_data = dataset_new.map(lambda x, y: (x / 255.0, y))\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "dataset_size = tf.data.experimental.cardinality(scaled_data).numpy()\n",
        "train_size = int(0.8 * dataset_size)\n",
        "test_size = dataset_size - train_size\n",
        "\n",
        "train = scaled_data.take(train_size)\n",
        "test = scaled_data.skip(train_size).take(test_size)\n",
        "\n",
        "# Define the VGG16 model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
        "    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
        "    Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
        "    Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(512, kernel_size=(3, 3), activation='relu'),\n",
        "    Conv2D(512, kernel_size=(3, 3), activation='relu'),\n",
        "    Conv2D(512, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(512, kernel_size=(3, 3), activation='relu'),\n",
        "    Conv2D(512, kernel_size=(3, 3), activation='relu'),\n",
        "    Conv2D(512, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(4096, activation='relu'),\n",
        "    Dense(4096, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "hist = model.fit(train, validation_data=test, epochs=5)\n",
        "\n",
        "# Model evaluation metrics\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Evaluate the model\n",
        "for batch in test.as_numpy_iterator():\n",
        "    X, y = batch\n",
        "    y_pred_batch = model.predict(X)\n",
        "    y_true.extend(y)\n",
        "    y_pred.extend(np.argmax(y_pred_batch, axis=1))\n",
        "\n",
        "# Convert y_true and y_pred to numpy arrays\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "# Calculate confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(f'Confusion Matrix:\\n{cm}')\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Calculate and print AUC, precision, recall, and F1 score\n",
        "auc = roc_auc_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average='macro')\n",
        "recall = recall_score(y_true, y_pred, average='macro')\n",
        "f1 = f1_score(y_true, y_pred, average='macro')\n",
        "classification_rep = classification_report(y_true, y_pred, target_names=class_labels)\n",
        "\n",
        "print(f'AUC: {auc}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n",
        "print(f'Classification Report:\\n{classification_rep}')\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "fig = plt.figure()\n",
        "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
        "plt.plot(hist.history['val_accuracy'], color='blue', label='val_accuracy')\n",
        "fig.suptitle('ACCURACY', fontsize=20)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "# Plot training and validation loss\n",
        "fig = plt.figure()\n",
        "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
        "plt.plot(hist.history['val_loss'], color='blue', label='val_loss')\n",
        "fig.suptitle('LOSS', fontsize=20)\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xXZ44bl0FUjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLOv8 Model**"
      ],
      "metadata": {
        "id": "MYg8uT6EIdWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mMNbyJnVJXGl",
        "outputId": "6f3d3b5a-61d6-4bd4-d8a6-908173dc35ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annotated images with bounding boxes saved in the 'images' directory.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Function to read YOLO annotations\n",
        "def read_yolo_annotations(annotation_file, img_width, img_height):\n",
        "    boxes = []\n",
        "    with open(annotation_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            class_id = int(parts[0])\n",
        "            x_center = float(parts[1])\n",
        "            y_center = float(parts[2])\n",
        "            width = float(parts[3])\n",
        "            height = float(parts[4])\n",
        "            x_min = int((x_center - width / 2) * img_width)\n",
        "            y_min = int((y_center - height / 2) * img_height)\n",
        "            x_max = int((x_center + width / 2) * img_width)\n",
        "            y_max = int((y_center + height / 2) * img_height)\n",
        "            boxes.append((class_id, x_min, y_min, x_max, y_max))\n",
        "    return boxes\n",
        "\n",
        "# Directory paths\n",
        "input_dir = '/content/drive/MyDrive/yolo_dataset/yolo_dataset'\n",
        "output_dir = os.path.join(input_dir, 'images')\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# List all image files in the input directory\n",
        "image_files = [f for f in os.listdir(input_dir) if f.endswith('.PNG')]\n",
        "\n",
        "# Loop through each image file\n",
        "for file_name in image_files:\n",
        "    # Load image\n",
        "    img_path = os.path.join(input_dir, file_name)\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is not None:\n",
        "        img_height, img_width = image.shape[:2]\n",
        "\n",
        "        # Read annotations\n",
        "        annotation_path = os.path.join(input_dir, file_name.replace('.PNG', '.txt'))\n",
        "        if os.path.exists(annotation_path):\n",
        "            boxes = read_yolo_annotations(annotation_path, img_width, img_height)\n",
        "\n",
        "            # Draw bounding boxes and labels\n",
        "            for (class_id, x_min, y_min, x_max, y_max) in boxes:\n",
        "                color = (0, 255, 0)  # Green color for bounding boxes\n",
        "                cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, 2)\n",
        "                label = \"Motorcycles\" if class_id == 1 else \"Saloon Cars\"  # Assigning class labels based on class_id\n",
        "                cv2.putText(image, label, (x_min, y_min - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "            # Save the annotated image in the output directory\n",
        "            output_path = os.path.join(output_dir, f\"images_{file_name}\")\n",
        "            cv2.imwrite(output_path, image)\n",
        "\n",
        "print(\"Annotated images with bounding boxes saved in the 'images' directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geigcAK1QjVQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMD3eqDVzARn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqsDnOQWb_SP",
        "outputId": "bd50ba61-33c7-4320-bd26-2dd9155d3a15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annotations saved in the 'label' directory.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "# Function to read YOLO annotations\n",
        "def read_yolo_annotations(annotation_file, img_width, img_height):\n",
        "    boxes = []\n",
        "    with open(annotation_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            class_id = int(parts[0])\n",
        "            x_center = float(parts[1])\n",
        "            y_center = float(parts[2])\n",
        "            width = float(parts[3])\n",
        "            height = float(parts[4])\n",
        "            x_min = int((x_center - width / 2) * img_width)\n",
        "            y_min = int((y_center - height / 2) * img_height)\n",
        "            x_max = int((x_center + width / 2) * img_width)\n",
        "            y_max = int((y_center + height / 2) * img_height)\n",
        "            boxes.append((class_id, x_min, y_min, x_max, y_max))\n",
        "    return boxes\n",
        "\n",
        "# Directory paths\n",
        "input_dir = '/content/drive/MyDrive/yolo_dataset/yolo_dataset'\n",
        "output_annotation_dir = os.path.join(input_dir, 'labels')\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_annotation_dir):\n",
        "    os.makedirs(output_annotation_dir)\n",
        "\n",
        "# List all image files in the input directory\n",
        "image_files = [f for f in os.listdir(input_dir) if f.endswith('.PNG')]\n",
        "\n",
        "# Loop through each image file\n",
        "for file_name in image_files:\n",
        "    # Read annotations\n",
        "    annotation_path = os.path.join(input_dir, file_name.replace('.PNG', '.txt'))\n",
        "    if os.path.exists(annotation_path):\n",
        "        img = cv2.imread(os.path.join(input_dir, file_name))\n",
        "        img_height, img_width = img.shape[:2]\n",
        "        boxes = read_yolo_annotations(annotation_path, img_width, img_height)\n",
        "\n",
        "        # Save annotations to annotation directory\n",
        "        annotation_output_path = os.path.join(output_annotation_dir, f\"{file_name.replace('.PNG', '.txt')}\")\n",
        "        with open(annotation_output_path, 'w') as annotation_file:\n",
        "            for (class_id, x_min, y_min, x_max, y_max) in boxes:\n",
        "                annotation_line = f\"{class_id} {x_min} {y_min} {x_max - x_min} {y_max - y_min}\\n\"\n",
        "                annotation_file.write(annotation_line)\n",
        "\n",
        "print(\"Annotations saved in the 'label' directory.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgCVylxv4GBE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-rVxlUN6XM_",
        "outputId": "902ae95e-be89-40f5-c91d-63fe19eb5c0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images loaded: 2203\n",
            "Annotations loaded: 441\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "from torchvision.transforms import functional as F\n",
        "\n",
        "# Function to read YOLO annotations\n",
        "def read_yolo_annotations(annotation_file, img_width, img_height):\n",
        "    boxes = []\n",
        "    with open(annotation_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            parts = line.strip().split()\n",
        "            class_id = int(parts[0])\n",
        "            x_center = float(parts[1])\n",
        "            y_center = float(parts[2])\n",
        "            width = float(parts[3])\n",
        "            height = float(parts[4])\n",
        "            x_min = int((x_center - width / 2) * img_width)\n",
        "            y_min = int((y_center - height / 2) * img_height)\n",
        "            x_max = int((x_center + width / 2) * img_width)\n",
        "            y_max = int((y_center + height / 2) * img_height)\n",
        "            boxes.append((class_id, x_min, y_min, x_max, y_max))\n",
        "    return boxes\n",
        "\n",
        "# Function to load images, resize them, and normalize pixel values\n",
        "def load_images_and_annotations(input_dir):\n",
        "    images = []\n",
        "    annotations = []\n",
        "\n",
        "    # List all image files in the input directory\n",
        "    image_files = [f for f in os.listdir(input_dir) if f.endswith('.PNG')]\n",
        "\n",
        "    for file_name in image_files:\n",
        "        img_path = os.path.join(input_dir, file_name)\n",
        "        annotation_path = os.path.join(input_dir, file_name.replace('.PNG', '.txt'))\n",
        "\n",
        "        # Read image\n",
        "        image = cv2.imread(img_path)\n",
        "        if image is not None:\n",
        "            img_height, img_width = image.shape[:2]\n",
        "\n",
        "            # Resize image to desired size\n",
        "            image = cv2.resize(image, (416, 416))\n",
        "\n",
        "            # Normalize pixel values\n",
        "            image = F.to_tensor(image)\n",
        "            image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "            # Read annotations\n",
        "            if os.path.exists(annotation_path):\n",
        "                boxes = read_yolo_annotations(annotation_path, img_width, img_height)\n",
        "                annotations.append(boxes)\n",
        "\n",
        "            images.append(image)\n",
        "\n",
        "    return images, annotations\n",
        "\n",
        "# Directory path\n",
        "input_dir = '/content/drive/MyDrive/yolo_dataset/yolo_dataset'\n",
        "\n",
        "# Load images and annotations\n",
        "images, annotations = load_images_and_annotations(input_dir)\n",
        "\n",
        "print(\"Images loaded:\", len(images))\n",
        "print(\"Annotations loaded:\", len(annotations))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCR3SmdQ6XQQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlEzyD5GgyCy",
        "outputId": "901bc96b-3f26-46a6-a890-05d5097b2e96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annotated data split into training and testing sets.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define the directory paths\n",
        "base_directory = '/content/drive/MyDrive/yolo_dataset/yolo_dataset'\n",
        "image_directory = os.path.join(base_directory, 'images')\n",
        "train_image_directory = os.path.join(image_directory, 'train')\n",
        "test_image_directory = os.path.join(image_directory, 'test')\n",
        "\n",
        "# Create train and test directories if they don't exist\n",
        "os.makedirs(train_image_directory, exist_ok=True)\n",
        "os.makedirs(test_image_directory, exist_ok=True)\n",
        "\n",
        "# List all annotated files (assuming annotations are in .PNG format)\n",
        "annotation_files = [f for f in os.listdir(image_directory) if f.endswith('.PNG')]\n",
        "\n",
        "# Shuffle the files\n",
        "random.shuffle(annotation_files)\n",
        "\n",
        "# Define the split ratio (e.g., 80% train, 20% test)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(annotation_files) * split_ratio)\n",
        "\n",
        "# Split the files into training and testing sets\n",
        "train_files = annotation_files[:split_index]\n",
        "test_files = annotation_files[split_index:]\n",
        "\n",
        "# Move the files to the respective directories\n",
        "for file_name in train_files:\n",
        "    src_path = os.path.join(image_directory, file_name)\n",
        "    dst_path = os.path.join(train_directory, file_name)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "for file_name in test_files:\n",
        "    src_path = os.path.join(image_directory, file_name)\n",
        "    dst_path = os.path.join(test_image_directory, file_name)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "print(\"Annotated data split into training and testing sets.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMQLXkZdjPJC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91TB8wR9VD4H",
        "outputId": "501976eb-79d6-4ead-e6bd-7cdc5d6cdc24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annotations split into training and testing sets.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "# Define the directory paths\n",
        "yolo_labels_directory_directory = '/content/drive/MyDrive/yolo_dataset/yolo_dataset'\n",
        "label_directory = os.path.join(yolo_labels_directory_directory, 'labels')\n",
        "train_label_directory = os.path.join(label_directory, 'train')\n",
        "test_label_directory = os.path.join(yolo_labels_directory_directory, 'test')\n",
        "\n",
        "#yolo_labels_directory_directory = '/content/drive/MyDrive/yolo_dataset/yolo_dataset/labels'\n",
        "#train_labels_directory = '/content/drive/MyDrive/yolo_dataset/yolo_dataset/train_labels'\n",
        "#test_labels_directory = '/content/drive/MyDrive/yolo_dataset/yolo_dataset/test_labels'\n",
        "\n",
        "# Create train and test directories if they don't exist\n",
        "os.makedirs(train_label_directory, exist_ok=True)\n",
        "os.makedirs(test_label_directory, exist_ok=True)\n",
        "\n",
        "# List all annotation files\n",
        "label_files = [f for f in os.listdir(yolo_labels_directory_directory ) if f.endswith('.txt')]\n",
        "\n",
        "# Shuffle the files\n",
        "random.shuffle(label_files)\n",
        "\n",
        "# Define the split ratio (e.g., 80% train, 20% test)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(label_files) * split_ratio)\n",
        "\n",
        "# Split the files into training and testing sets\n",
        "train_files = label_files[:split_index]\n",
        "test_files = label_files[split_index:]\n",
        "\n",
        "# Move the files to the respective directories\n",
        "for file_name in train_files:\n",
        "    src_path = os.path.join(yolo_labels_directory_directory , file_name)\n",
        "    dst_path = os.path.join(train_label_directory, file_name)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "for file_name in test_files:\n",
        "    src_path = os.path.join(yolo_labels_directory_directory , file_name)\n",
        "    dst_path = os.path.join(yolo_labels_directory_directory , file_name)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "print(\"Annotations split into training and testing sets.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s07YBP4DVD8h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSD1cdL0vvRH"
      },
      "outputs": [],
      "source": [
        "# Define the root directory of your YOLO dataset\n",
        "dataset_root = '/content/drive/MyDrive/yolo_dataset/yolo_dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1MAI439v-wF"
      },
      "outputs": [],
      "source": [
        "# Construct the paths for images and labels directories\n",
        "train_images_directory = os.path.join(dataset_root, 'images')\n",
        "train_labels_directory = os.path.join(dataset_root, 'labels')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j2U3RL_v_DG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s6Ir2kWnlgNj"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Function to load images from a directory\n",
        "def load_images_from_dir(directory):\n",
        "    images = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
        "            img_path = os.path.join(directory, filename)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                images.append(img)\n",
        "    return np.array(images)\n",
        "\n",
        "# Function to load labels from a directory\n",
        "def load_labels_from_dir(directory):\n",
        "    labels = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            label_path = os.path.join(directory, filename)\n",
        "            with open(label_path, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                # Assuming each line in the label file contains the class and bounding box coordinates\n",
        "                label = [[float(x) for x in line.strip().split()] for line in lines]\n",
        "                labels.extend(label)  # Use extend instead of append\n",
        "    return np.array(labels)\n",
        "\n",
        "\n",
        "# Define the directory paths for images and labels\n",
        "train_images_directory = '/content/drive/MyDrive/yolo_dataset/yolo_dataset/train_images'\n",
        "train_labels_directory = '/content/drive/MyDrive/yolo_dataset/yolo_dataset/train_labels'\n",
        "\n",
        "# Load training images and labels\n",
        "train_images = load_images_from_dir(train_images_directory)\n",
        "train_labels = load_labels_from_dir(train_labels_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sykssak__nf9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j3KCGWeU_n24"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N6zU3Wy3WXK"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Define the dataset configuration\n",
        "dataset_config = {\n",
        "    'train': {\n",
        "        'images': '/content/drive/MyDrive/yolo_dataset/yolo_dataset/images/train',\n",
        "        'labels': '/content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/train'\n",
        "    },\n",
        "    'val': {\n",
        "        'images': '/content/drive/MyDrive/yolo_dataset/yolo_dataset/images/train',\n",
        "        'labels': '/content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/train'\n",
        "    },\n",
        "    'nc': 2,  # Number of classes\n",
        "    'names': ['Motorcycles', 'Saloon Cars']  # Class names\n",
        "}\n",
        "\n",
        "# Define the directory path for the YAML file\n",
        "directory = '/content/drive/MyDrive/yolo_dataset'\n",
        "yaml_file = 'config_file.yaml'\n",
        "yaml_file_path = os.path.join(directory, yaml_file)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Save dataset configuration to YAML file\n",
        "with open(yaml_file_path, 'w') as file:\n",
        "    yaml.dump(dataset_config, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCCMLQpJ3Wd8"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "import os\n",
        "\n",
        "# Define YOLOv5 model configuration\n",
        "yolov5_config = {\n",
        "    'model': {\n",
        "        'type': 'YOLOv5',  # YOLOv5 type\n",
        "        'nc': 2,  # Number of classes\n",
        "        'depth_multiple': 0.33,  # Model depth multiple\n",
        "        'width_multiple': 0.50,  # Model width multiple\n",
        "    },\n",
        "    'train': {\n",
        "        'yaml_file': '/content/drive/MyDrive/yolo_dataset/yolov5_config_file.yaml',  # Path to dataset YAML file\n",
        "        'batch_size': 16,  # Batch size for training\n",
        "        'epochs': 10,  # Number of epochs\n",
        "        'weights': 'yolov5s.pt',  # Path to initial weights file (optional)\n",
        "        'img_size': [416, 416],  # Image size for training\n",
        "        'device': '0',  # Device for training (GPU index)\n",
        "        'multi_scale': False,  # Use multi-scale training\n",
        "        'cache_images': True,  # Cache images for faster training\n",
        "    },\n",
        "    'val': {\n",
        "        'yaml_file': '/content/drive/MyDrive/yolo_dataset/yolov5_config_file.yaml',  # Path to dataset YAML file\n",
        "        'batch_size': 16,  # Batch size for testing\n",
        "        'img_size': [416, 416],  # Image size for testing\n",
        "        'conf_thres': 0.001,  # Confidence threshold for testing\n",
        "        'iou_thres': 0.6,  # IoU threshold for testing\n",
        "        'device': '0',  # Device for testing (GPU index)\n",
        "        'augment': False,  # Augment testing\n",
        "    },\n",
        "    'export': {\n",
        "        'weights': 'yolov5s_final.pt',  # Path to exported final weights file\n",
        "        'img_size': [416, 416],  # Image size for exporting\n",
        "        'batch_size': 1,  # Batch size for exporting\n",
        "    },\n",
        "}\n",
        "\n",
        "# Define the directory path for the YAML file\n",
        "directory = '/content/drive/MyDrive/yolo_dataset'\n",
        "yolov5_yaml_file = 'yolov5_config_file.yaml'\n",
        "yolov5_yaml_file_path = os.path.join(directory, yolov5_yaml_file)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Save YOLOv5 model configuration to YAML file\n",
        "with open(yolov5_yaml_file_path, 'w') as file:\n",
        "    yaml.dump(yolov5_config, file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy134RylF2SB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WayInz6dA6Vm"
      },
      "outputs": [],
      "source": [
        "import yaml\n",
        "\n",
        "# Path to dataset configuration YAML file\n",
        "dataset_config_file = '/content/drive/MyDrive/yolo_dataset/config_file.yaml'\n",
        "yolov5_config_file = '/content/drive/MyDrive/yolo_dataset/yolov5_config_file.yaml'\n",
        "\n",
        "# Read dataset configuration from YAML file\n",
        "with open(dataset_config_file, 'r') as file:\n",
        "    dataset_config = yaml.safe_load(file)\n",
        "\n",
        "# Read YOLOv5 configuration from YAML file\n",
        "with open(yolov5_config_file, 'r') as file:\n",
        "    yolov5_config = yaml.safe_load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGTOm8wFTdVL",
        "outputId": "4f7c8ecd-232c-40cf-fd19-9f87ab102360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data.yaml file created successfully at /content/drive/MyDrive/yolo_dataset/yolo_dataset/data.yaml.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "# Define the dataset configuration\n",
        "dataset_config = {\n",
        "    'train': {\n",
        "        'images': '/content/drive/MyDrive/yolo_dataset/yolo_dataset/images/train',\n",
        "        'labels': '/content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/train'\n",
        "    },\n",
        "    'val': {\n",
        "        'images': '/content/drive/MyDrive/yolo_dataset/yolo_dataset/images/train',\n",
        "        'labels': '/content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/train'\n",
        "    },\n",
        "    'nc': 2,  # Number of classes\n",
        "    'names': ['Motorcycles', 'Saloon Cars']  # Class names\n",
        "}\n",
        "\n",
        "# Specify the directory where you want to save the YAML file\n",
        "ROOT_DIR = '/content/drive/MyDrive/yolo_dataset/yolo_dataset'  # Adjust this path if needed\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "\n",
        "# Save the configuration to a YAML file\n",
        "config_file_path = os.path.join(ROOT_DIR, 'data.yaml')\n",
        "with open(config_file_path, 'w') as file:\n",
        "    yaml.dump(dataset_config, file, default_flow_style=False)\n",
        "\n",
        "print(f\"data.yaml file created successfully at {config_file_path}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsGDmuyi87QV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHqS4Wzy8NYu"
      },
      "outputs": [],
      "source": [
        "ROOT_DIR = '/content/drive/MyDrive/yolo_dataset/yolo_dataset'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9wbB4OW8Nej",
        "outputId": "77866981-e168-48a8-bf3a-b962798787a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.2.29-py3-none-any.whl (780 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Collecting ultralytics-thop>=0.2.5 (from ultralytics)\n",
            "  Downloading ultralytics_thop-0.2.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.29 ultralytics-thop-0.2.7\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTQrHH8e8Nhp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnRdcsszEIS3"
      },
      "outputs": [],
      "source": [
        "# Load a model\n",
        "model = YOLO(\"yolov8n.yaml\")  # build a new model from scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrd0PPIvBsCY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSanvKWJWAXV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdMnkd-bBowm",
        "outputId": "28ea1230-ca53-4ed4-c43c-8e7f79b0b64c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.29 🚀 Python-3.10.12 torch-2.3.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=/content/drive/MyDrive/yolo_dataset/yolo_dataset/data.yaml, epochs=1, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train20, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train20\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "YOLOv8n summary: 225 layers, 3011238 parameters, 3011222 gradients, 8.2 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train20', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/train.cache... 0 images, 1762 backgrounds, 0 corrupt: 100%|██████████| 1762/1762 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ No labels found in /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/train.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/train.cache... 0 images, 1762 backgrounds, 0 corrupt: 100%|██████████| 1762/1762 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ No labels found in /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/train.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
            "Plotting labels to runs/detect/train20/labels.jpg... \n",
            "zero-size array to reduction operation maximum which has no identity\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 640 train, 640 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train20\u001b[0m\n",
            "Starting training for 1 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "        1/1         0G          0      92.91          0          0        640: 100%|██████████| 111/111 [26:52<00:00, 14.53s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 56/56 [07:08<00:00,  7.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1762          0          0          0          0          0\n",
            "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1 epochs completed in 0.570 hours.\n",
            "Optimizer stripped from runs/detect/train20/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train20/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train20/weights/best.pt...\n",
            "Ultralytics YOLOv8.2.29 🚀 Python-3.10.12 torch-2.3.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 56/56 [06:29<00:00,  6.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all       1762          0          0          0          0          0\n",
            "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Speed: 2.3ms preprocess, 196.6ms inference, 0.0ms loss, 6.6ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train20\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Use the model\n",
        "results = model.train(data=os.path.join(ROOT_DIR, \"data.yaml\"), epochs=1)  # train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "56TL-dSO8b6r"
      },
      "outputs": [],
      "source": [
        "!scp -r /content/runs '/content/drive/MyDrive/yolo_dataset/object_train_data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1CP2Lahi8b94"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFD9_jOx8cF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e511d06e-f97b-410d-e238-aafc38d2cf22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotated data split into training and testing sets.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define the directory paths\n",
        "base_directory = '/content/drive/MyDrive/yolo_dataset/yolo_dataset'\n",
        "image_directory = os.path.join(base_directory, 'images')\n",
        "label_directory = os.path.join(base_directory, 'labels')\n",
        "\n",
        "train_image_directory = os.path.join(image_directory, 'train')\n",
        "test_image_directory = os.path.join(image_directory, 'test')\n",
        "train_label_directory = os.path.join(label_directory, 'train')\n",
        "test_label_directory = os.path.join(label_directory, 'test')\n",
        "\n",
        "# Create train and test directories if they don't exist\n",
        "os.makedirs(train_image_directory, exist_ok=True)\n",
        "os.makedirs(test_image_directory, exist_ok=True)\n",
        "os.makedirs(train_label_directory, exist_ok=True)\n",
        "os.makedirs(test_label_directory, exist_ok=True)\n",
        "\n",
        "# List all image and label files in the input directory\n",
        "image_files = [f for f in os.listdir(image_directory) if f.endswith('.PNG')]\n",
        "label_files = [f for f in os.listdir(label_directory) if f.endswith('.txt')]\n",
        "\n",
        "# Shuffle the files\n",
        "random.shuffle(image_files)\n",
        "\n",
        "# Define the split ratio (e.g., 80% train, 20% test)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(image_files) * split_ratio)\n",
        "\n",
        "# Split the files into training and testing sets\n",
        "train_image_files = image_files[:split_index]\n",
        "test_image_files = image_files[split_index:]\n",
        "\n",
        "# Move the image files to the respective directories\n",
        "for file_name in train_image_files:\n",
        "    src_path = os.path.join(image_directory, file_name)\n",
        "    dst_path = os.path.join(train_image_directory, file_name)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "for file_name in test_image_files:\n",
        "    src_path = os.path.join(image_directory, file_name)\n",
        "    dst_path = os.path.join(test_image_directory, file_name)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move the label files to the respective directories\n",
        "for file_name in train_image_files:\n",
        "    label_name = file_name.replace('.PNG', '.txt')\n",
        "    src_path = os.path.join(label_directory, label_name)\n",
        "    dst_path = os.path.join(train_label_directory, label_name)\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.move(src_path, dst_path)\n",
        "\n",
        "for file_name in test_image_files:\n",
        "    label_name = file_name.replace('.PNG', '.txt')\n",
        "    src_path = os.path.join(label_directory, label_name)\n",
        "    dst_path = os.path.join(test_label_directory, label_name)\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.move(src_path, dst_path)\n",
        "\n",
        "print(\"Annotated data split into training and testing sets.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2aGIihq8NoC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874cdad7-6cde-45bd-bce6-29c0d652df91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Annotated data split into training and testing sets.\n",
            "data.yaml file created successfully at /content/drive/MyDrive/yolo_dataset/data.yaml.\n",
            "yolov8_config_file.yaml file created successfully at /content/drive/MyDrive/yolo_dataset/yolov8_config_file.yaml.\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.30)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: ultralytics-thop>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.2.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.5.40)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Ultralytics YOLOv8.2.30 🚀 Python-3.10.12 torch-2.3.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=/content/drive/MyDrive/yolo_dataset/data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=416, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "YOLOv8n summary: 225 layers, 3011238 parameters, 3011222 gradients, 8.2 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/train... 2 images, 1830 backgrounds, 0 corrupt: 100%|██████████| 1832/1832 [00:39<00:00, 45.88it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/train.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/test... 0 images, 89 backgrounds, 0 corrupt: 100%|██████████| 89/89 [00:00<00:00, 174.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ No labels found in /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/test.cache. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/test.cache\n",
            "WARNING ⚠️ No labels found in /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/test.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
            "Plotting labels to runs/detect/train3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 416 train, 416 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 10 epochs...\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       1/10         0G  0.0005133      39.64  0.0002935          0        416: 100%|██████████| 115/115 [13:20<00:00,  6.96s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:12<00:00,  4.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         89          0          0          0          0          0\n",
            "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       2/10         0G     0.1096      21.87     0.0592          0        416: 100%|██████████| 115/115 [11:59<00:00,  6.25s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:11<00:00,  3.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         89          0          0          0          0          0\n",
            "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       3/10         0G      0.106      9.347     0.0727          0        416: 100%|██████████| 115/115 [12:17<00:00,  6.42s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:11<00:00,  3.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         89          0          0          0          0          0\n",
            "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "       4/10         0G    0.07433      3.219    0.04641          0        416: 100%|██████████| 115/115 [12:34<00:00,  6.57s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:10<00:00,  3.34s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "torch.cat(): expected a non-empty list of Tensors",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-c8e467c320fb>\u001b[0m in \u001b[0;36m<cell line: 149>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_yaml_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgsz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m416\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    422\u001b[0m                 \u001b[0;31m# Validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_epoch\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_stop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_loss_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcontain\u001b[0m \u001b[0;34m\"fitness\"\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \"\"\"\n\u001b[0;32m--> 565\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m         \u001b[0mfitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fitness\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# use loss as fitness measure if not found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_fitness\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_fitness\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mfitness\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/engine/validator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, trainer, model)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_val_batch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/models/yolo/detect/val.py\u001b[0m in \u001b[0;36mget_stats\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;34m\"\"\"Returns metrics statistics and results dictionary.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# to numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnt_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_cls\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnt_per_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/models/yolo/detect/val.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;34m\"\"\"Returns metrics statistics and results dictionary.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# to numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnt_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_cls\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnt_per_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target_img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
          ]
        }
      ],
      "source": [
        "# Step 1: Preparing the Dataset\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define the directory paths\n",
        "base_directory = '/content/drive/MyDrive/yolo_dataset/yolo_dataset'\n",
        "image_directory = os.path.join(base_directory, 'images')\n",
        "label_directory = os.path.join(base_directory, 'labels')\n",
        "\n",
        "train_image_directory = os.path.join(image_directory, 'train')\n",
        "test_image_directory = os.path.join(image_directory, 'test')\n",
        "train_label_directory = os.path.join(label_directory, 'train')\n",
        "test_label_directory = os.path.join(label_directory, 'test')\n",
        "\n",
        "# Create train and test directories if they don't exist\n",
        "os.makedirs(train_image_directory, exist_ok=True)\n",
        "os.makedirs(test_image_directory, exist_ok=True)\n",
        "os.makedirs(train_label_directory, exist_ok=True)\n",
        "os.makedirs(test_label_directory, exist_ok=True)\n",
        "\n",
        "# List all image and label files in the input directory\n",
        "image_files = [f for f in os.listdir(image_directory) if f.endswith('.PNG')]\n",
        "label_files = [f for f in os.listdir(label_directory) if f.endswith('.txt')]\n",
        "\n",
        "# Shuffle the files\n",
        "random.shuffle(image_files)\n",
        "\n",
        "# Define the split ratio (e.g., 80% train, 20% test)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(image_files) * split_ratio)\n",
        "\n",
        "# Split the files into training and testing sets\n",
        "train_image_files = image_files[:split_index]\n",
        "test_image_files = image_files[split_index:]\n",
        "\n",
        "# Move the image files to the respective directories\n",
        "for file_name in train_image_files:\n",
        "    src_path = os.path.join(image_directory, file_name)\n",
        "    dst_path = os.path.join(train_image_directory, file_name)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "for file_name in test_image_files:\n",
        "    src_path = os.path.join(image_directory, file_name)\n",
        "    dst_path = os.path.join(test_image_directory, file_name)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move the label files to the respective directories\n",
        "for file_name in train_image_files:\n",
        "    label_name = file_name.replace('.PNG', '.txt')\n",
        "    src_path = os.path.join(label_directory, label_name)\n",
        "    dst_path =    os.path.join(train_label_directory, label_name)\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.move(src_path, dst_path)\n",
        "\n",
        "for file_name in test_image_files:\n",
        "    label_name = file_name.replace('.PNG', '.txt')\n",
        "    src_path = os.path.join(label_directory, label_name)\n",
        "    dst_path = os.path.join(test_label_directory, label_name)\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.move(src_path, dst_path)\n",
        "\n",
        "print(\"Annotated data split into training and testing sets.\")\n",
        "\n",
        "# Step 2: Creating Configuration Files\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "# Define the dataset configuration\n",
        "dataset_config = {\n",
        "    'train': os.path.join(base_directory, 'images/train'),\n",
        "    'val': os.path.join(base_directory, 'images/test'),\n",
        "    'nc': 2,  # Number of classes\n",
        "    'names': ['Motorcycles', 'Saloon Cars']  # Class names\n",
        "}\n",
        "\n",
        "# Specify the directory where you want to save the YAML file\n",
        "ROOT_DIR = '/content/drive/MyDrive/yolo_dataset'\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "\n",
        "# Save the configuration to a YAML file\n",
        "config_file_path = os.path.join(ROOT_DIR, 'data.yaml')\n",
        "with open(config_file_path, 'w') as file:\n",
        "    yaml.dump(dataset_config, file, default_flow_style=False)\n",
        "\n",
        "print(f\"data.yaml file created successfully at {config_file_path}.\")\n",
        "\n",
        "# Define YOLOv8 model configuration\n",
        "yolov8_config = {\n",
        "    'model': {\n",
        "        'type': 'YOLOv8',  # YOLOv8 type\n",
        "        'nc': 2,  # Number of classes\n",
        "        'depth_multiple': 0.33,  # Model depth multiple\n",
        "        'width_multiple': 0.50,  # Model width multiple\n",
        "    },\n",
        "    'train': {\n",
        "        'yaml_file': os.path.join(ROOT_DIR, 'data.yaml'),  # Path to dataset YAML file\n",
        "        'batch_size': 16,  # Batch size for training\n",
        "        'epochs': 10,  # Number of epochs\n",
        "        'weights': 'yolov8n.pt',  # Path to initial weights file (optional)\n",
        "        'img_size': [416, 416],  # Image size for training\n",
        "        'device': '0',  # Device for training (GPU index)\n",
        "        'multi_scale': False,  # Use multi-scale training\n",
        "        'cache_images': True,  # Cache images for faster training\n",
        "    },\n",
        "    'val': {\n",
        "        'yaml_file': os.path.join(ROOT_DIR, 'data.yaml'),  # Path to dataset YAML file\n",
        "        'batch_size': 16,  # Batch size for testing\n",
        "        'img_size': [416, 416],  # Image size for testing\n",
        "        'conf_thres': 0.001,  # Confidence threshold for testing\n",
        "        'iou_thres': 0.6,  # IoU threshold for testing\n",
        "        'device': '0',  # Device for testing (GPU index)\n",
        "        'augment': False,  # Augment testing\n",
        "    },\n",
        "    'export': {\n",
        "        'weights': 'yolov8n_final.pt',  # Path to exported final weights file\n",
        "        'img_size': [416, 416],  # Image size for exporting\n",
        "        'batch_size': 1,  # Batch size for exporting\n",
        "    },\n",
        "}\n",
        "\n",
        "# Define the directory path for the YAML file\n",
        "yolov8_yaml_file = 'yolov8_config_file.yaml'\n",
        "yolov8_yaml_file_path = os.path.join(ROOT_DIR, yolov8_yaml_file)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "\n",
        "# Save YOLOv8 model configuration to YAML file\n",
        "with open(yolov8_yaml_file_path, 'w') as file:\n",
        "    yaml.dump(yolov8_config, file, default_flow_style=False)\n",
        "\n",
        "print(f\"yolov8_config_file.yaml file created successfully at {yolov8_yaml_file_path}.\")\n",
        "\n",
        "# Step 3: Training the YOLOv8 Model\n",
        "!pip install ultralytics\n",
        "\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Define the paths\n",
        "ROOT_DIR = '/content/drive/MyDrive/yolo_dataset'\n",
        "data_yaml_path = os.path.join(ROOT_DIR, 'data.yaml')\n",
        "\n",
        "# Load a YOLOv8 model\n",
        "model = YOLO(\"yolov8n.yaml\")  # you can also specify a pre-trained model here\n",
        "\n",
        "# Train the model\n",
        "results = model.train(data=data_yaml_path, epochs=10, imgsz=416, batch=16)\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = model.val(data=data_yaml_path, imgsz=416, batch=16)\n",
        "\n",
        "print(\"Training and evaluation completed.\")\n",
        "print(\"Results:\", metrics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--SnitzL8NsU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82_faf_SGzol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6880a0-60d1-4864-ddbd-79dd2c40d5f3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Annotated data split into training and testing sets.\n",
            "data.yaml file created successfully at /content/drive/MyDrive/yolo_dataset/data.yaml.\n",
            "yolov8_config_file.yaml file created successfully at /content/drive/MyDrive/yolo_dataset/yolov8_config_file.yaml.\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.10/dist-packages (8.2.30)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n",
            "Requirement already satisfied: ultralytics-thop>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.2.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics) (12.5.40)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Ultralytics YOLOv8.2.30 🚀 Python-3.10.12 torch-2.3.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.yaml, data=/content/drive/MyDrive/yolo_dataset/data.yaml, epochs=4, time=None, patience=100, batch=16, imgsz=416, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\n",
            "Overriding model.yaml nc=80 with nc=2\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    751702  ultralytics.nn.modules.head.Detect           [2, [64, 128, 256]]           \n",
            "YOLOv8n summary: 225 layers, 3011238 parameters, 3011222 gradients, 8.2 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/train.cache... 2 images, 1830 backgrounds, 0 corrupt: 100%|██████████| 1832/1832 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/test.cache... 0 images, 89 backgrounds, 0 corrupt: 100%|██████████| 89/89 [00:00<?, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING ⚠️ No labels found in /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/test.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train4/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
            "Image sizes 416 train, 416 val\n",
            "Using 0 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train4\u001b[0m\n",
            "Starting training for 4 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        1/4         0G     0.1259      39.47    0.07837          0        416: 100%|██████████| 115/115 [14:08<00:00,  7.38s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:12<00:00,  4.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         89          0          0          0          0          0\n",
            "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        2/4         0G    0.02329      22.92    0.01277          0        416: 100%|██████████| 115/115 [13:22<00:00,  6.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:12<00:00,  4.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         89          0          0          0          0          0\n",
            "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        3/4         0G     0.1499      12.33    0.08349          0        416: 100%|██████████| 115/115 [13:22<00:00,  6.98s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:12<00:00,  4.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         89          0          0          0          0          0\n",
            "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "        4/4         0G    0.04205      7.436     0.0316          0        416: 100%|██████████| 115/115 [13:18<00:00,  6.94s/it]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:12<00:00,  4.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         89          0          0          0          0          0\n",
            "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "4 epochs completed in 0.920 hours.\n",
            "Optimizer stripped from runs/detect/train4/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train4/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train4/weights/best.pt...\n",
            "Ultralytics YOLOv8.2.30 🚀 Python-3.10.12 torch-2.3.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:11<00:00,  4.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         89          0          0          0          0          0\n",
            "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speed: 1.3ms preprocess, 112.2ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train4\u001b[0m\n",
            "Ultralytics YOLOv8.2.30 🚀 Python-3.10.12 torch-2.3.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "YOLOv8n summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/test.cache... 0 images, 89 backgrounds, 0 corrupt: 100%|██████████| 89/89 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING ⚠️ No labels found in /content/drive/MyDrive/yolo_dataset/yolo_dataset/labels/test.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 6/6 [00:11<00:00,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   all         89          0          0          0          0          0\n",
            "WARNING ⚠️ no labels found in detect set, can not compute metrics without labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speed: 1.2ms preprocess, 106.3ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train42\u001b[0m\n",
            "Training and evaluation completed.\n",
            "Results: "
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DetMetrics' object has no attribute 'curves_results'. See valid attributes below.\n\n    This class is a utility class for computing detection metrics such as precision, recall, and mean average precision\n    (mAP) of an object detection model.\n\n    Args:\n        save_dir (Path): A path to the directory where the output plots will be saved. Defaults to current directory.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class. Defaults to False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (tuple of str): A tuple of strings that represents the names of the classes. Defaults to an empty tuple.\n\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot the precision-recall curves for each class.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (tuple of str): A tuple of strings that represents the names of the classes.\n        box (Metric): An instance of the Metric class for storing the results of the detection metrics.\n        speed (dict): A dictionary for storing the execution time of different parts of the detection process.\n\n    Methods:\n        process(tp, conf, pred_cls, target_cls): Updates the metric results with the latest batch of predictions.\n        keys: Returns a list of keys for accessing the computed detection metrics.\n        mean_results: Returns a list of mean values for the computed detection metrics.\n        class_result(i): Returns a list of values for the computed detection metrics for a specific class.\n        maps: Returns a dictionary of mean average precision (mAP) values for different IoU thresholds.\n        fitness: Computes the fitness score based on the computed detection metrics.\n        ap_class_index: Returns a list of class indices sorted by their average precision (AP) values.\n        results_dict: Returns a dictionary that maps detection metric keys to their computed values.\n        curves: TODO\n        curves_results: TODO\n    ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-994fa365840e>\u001b[0m in \u001b[0;36m<cell line: 155>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training and evaluation completed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/__init__.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ultralytics/utils/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;34m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"'{name}' object has no attribute '{attr}'. See valid attributes below.\\n{self.__doc__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DetMetrics' object has no attribute 'curves_results'. See valid attributes below.\n\n    This class is a utility class for computing detection metrics such as precision, recall, and mean average precision\n    (mAP) of an object detection model.\n\n    Args:\n        save_dir (Path): A path to the directory where the output plots will be saved. Defaults to current directory.\n        plot (bool): A flag that indicates whether to plot precision-recall curves for each class. Defaults to False.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered. Defaults to None.\n        names (tuple of str): A tuple of strings that represents the names of the classes. Defaults to an empty tuple.\n\n    Attributes:\n        save_dir (Path): A path to the directory where the output plots will be saved.\n        plot (bool): A flag that indicates whether to plot the precision-recall curves for each class.\n        on_plot (func): An optional callback to pass plots path and data when they are rendered.\n        names (tuple of str): A tuple of strings that represents the names of the classes.\n        box (Metric): An instance of the Metric class for storing the results of the detection metrics.\n        speed (dict): A dictionary for storing the execution time of different parts of the detection process.\n\n    Methods:\n        process(tp, conf, pred_cls, target_cls): Updates the metric results with the latest batch of predictions.\n        keys: Returns a list of keys for accessing the computed detection metrics.\n        mean_results: Returns a list of mean values for the computed detection metrics.\n        class_result(i): Returns a list of values for the computed detection metrics for a specific class.\n        maps: Returns a dictionary of mean average precision (mAP) values for different IoU thresholds.\n        fitness: Computes the fitness score based on the computed detection metrics.\n        ap_class_index: Returns a list of class indices sorted by their average precision (AP) values.\n        results_dict: Returns a dictionary that maps detection metric keys to their computed values.\n        curves: TODO\n        curves_results: TODO\n    "
          ]
        }
      ],
      "source": [
        "# Step 1: Preparing the Dataset\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Define the directory paths\n",
        "base_directory = '/content/drive/MyDrive/yolo_dataset/yolo_dataset'\n",
        "image_directory = os.path.join(base_directory, 'images')\n",
        "label_directory = os.path.join(base_directory, 'labels')\n",
        "\n",
        "train_image_directory = os.path.join(image_directory, 'train')\n",
        "test_image_directory = os.path.join(image_directory, 'test')\n",
        "train_label_directory = os.path.join(label_directory, 'train')\n",
        "test_label_directory = os.path.join(label_directory, 'test')\n",
        "\n",
        "# Create train and test directories if they don't exist\n",
        "os.makedirs(train_image_directory, exist_ok=True)\n",
        "os.makedirs(test_image_directory, exist_ok=True)\n",
        "os.makedirs(train_label_directory, exist_ok=True)\n",
        "os.makedirs(test_label_directory, exist_ok=True)\n",
        "\n",
        "# List all image and label files in the input directory\n",
        "image_files = [f for f in os.listdir(image_directory) if f.endswith('.PNG')]\n",
        "label_files = [f for f in os.listdir(label_directory) if f.endswith('.txt')]\n",
        "\n",
        "# Shuffle the files\n",
        "random.shuffle(image_files)\n",
        "\n",
        "# Define the split ratio (e.g., 80% train, 20% test)\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(image_files) * split_ratio)\n",
        "\n",
        "# Split the files into training and testing sets\n",
        "train_image_files = image_files[:split_index]\n",
        "test_image_files = image_files[split_index:]\n",
        "\n",
        "# Move the image files to the respective directories\n",
        "for file_name in train_image_files:\n",
        "    src_path = os.path.join(image_directory, file_name)\n",
        "    dst_path = os.path.join(train_image_directory, file_name)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "for file_name in test_image_files:\n",
        "    src_path = os.path.join(image_directory, file_name)\n",
        "    dst_path = os.path.join(test_image_directory, file_name)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move the label files to the respective directories\n",
        "for file_name in train_image_files:\n",
        "    label_name = file_name.replace('.PNG', '.txt')\n",
        "    src_path = os.path.join(label_directory, label_name)\n",
        "    dst_path =    os.path.join(train_label_directory, label_name)\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.move(src_path, dst_path)\n",
        "\n",
        "for file_name in test_image_files:\n",
        "    label_name = file_name.replace('.PNG', '.txt')\n",
        "    src_path = os.path.join(label_directory, label_name)\n",
        "    dst_path = os.path.join(test_label_directory, label_name)\n",
        "    if os.path.exists(src_path):\n",
        "        shutil.move(src_path, dst_path)\n",
        "\n",
        "print(\"Annotated data split into training and testing sets.\")\n",
        "\n",
        "# Step 2: Creating Configuration Files\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "# Define the dataset configuration\n",
        "dataset_config = {\n",
        "    'train': os.path.join(base_directory, 'images/train'),\n",
        "    'val': os.path.join(base_directory, 'images/test'),\n",
        "    'nc': 2,  # Number of classes\n",
        "    'names': ['Motorcycles', 'Saloon Cars']  # Class names\n",
        "}\n",
        "\n",
        "# Specify the directory where you want to save the YAML file\n",
        "ROOT_DIR = '/content/drive/MyDrive/yolo_dataset'\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "\n",
        "# Save the configuration to a YAML file\n",
        "config_file_path = os.path.join(ROOT_DIR, 'data.yaml')\n",
        "with open(config_file_path, 'w') as file:\n",
        "    yaml.dump(dataset_config, file, default_flow_style=False)\n",
        "\n",
        "print(f\"data.yaml file created successfully at {config_file_path}.\")\n",
        "\n",
        "# Define YOLOv8 model configuration\n",
        "yolov8_config = {\n",
        "    'model': {\n",
        "        'type': 'YOLOv8',  # YOLOv8 type\n",
        "        'nc': 2,  # Number of classes\n",
        "        'depth_multiple': 0.33,  # Model depth multiple\n",
        "        'width_multiple': 0.50,  # Model width multiple\n",
        "    },\n",
        "    'train': {\n",
        "        'yaml_file': os.path.join(ROOT_DIR, 'data.yaml'),  # Path to dataset YAML file\n",
        "        'batch_size': 16,  # Batch size for training\n",
        "        'epochs': 4,  # Number of epochs\n",
        "        'weights': 'yolov8n.pt',  # Path to initial weights file (optional)\n",
        "        'img_size': [416, 416],  # Image size for training\n",
        "        'device': '0',  # Device for training (GPU index)\n",
        "        'multi_scale': False,  # Use multi-scale training\n",
        "        'cache_images': True,  # Cache images for faster training\n",
        "    },\n",
        "    'val': {\n",
        "        'yaml_file': os.path.join(ROOT_DIR, 'data.yaml'),  # Path to dataset YAML file\n",
        "        'batch_size': 16,  # Batch size for testing\n",
        "        'img_size': [416, 416],  # Image size for testing\n",
        "        'conf_thres': 0.001,  # Confidence threshold for testing\n",
        "        'iou_thres': 0.6,  # IoU threshold for testing\n",
        "        'device': '0',  # Device for testing (GPU index)\n",
        "        'augment': False,  # Augment testing\n",
        "    },\n",
        "    'export': {\n",
        "        'weights': 'yolov8n_final.pt',  # Path to exported final weights file\n",
        "        'img_size': [416, 416],  # Image size for exporting\n",
        "        'batch_size': 1,  # Batch size for exporting\n",
        "    },\n",
        "}\n",
        "\n",
        "# Define the directory path for the YAML file\n",
        "yolov8_yaml_file = 'yolov8_config_file.yaml'\n",
        "yolov8_yaml_file_path = os.path.join(ROOT_DIR, yolov8_yaml_file)\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(ROOT_DIR, exist_ok=True)\n",
        "\n",
        "# Save YOLOv8 model configuration to YAML file\n",
        "with open(yolov8_yaml_file_path, 'w') as file:\n",
        "    yaml.dump(yolov8_config, file, default_flow_style=False)\n",
        "\n",
        "print(f\"yolov8_config_file.yaml file created successfully at {yolov8_yaml_file_path}.\")\n",
        "\n",
        "# Step 3: Training the YOLOv8 Model\n",
        "!pip install ultralytics\n",
        "\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Define the paths\n",
        "ROOT_DIR = '/content/drive/MyDrive/yolo_dataset'\n",
        "data_yaml_path = os.path.join(ROOT_DIR, 'data.yaml')\n",
        "\n",
        "# Load a YOLOv8 model\n",
        "model = YOLO(\"yolov8n.yaml\")  # you can also specify a pre-trained model here\n",
        "\n",
        "# Train the model\n",
        "results = model.train(data=data_yaml_path, epochs=4, imgsz=416, batch=16)\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = model.val(data=data_yaml_path, imgsz=416, batch=16)\n",
        "\n",
        "print(\"Training and evaluation completed.\")\n",
        "print(\"Results:\", metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eimFatJYGzsj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}